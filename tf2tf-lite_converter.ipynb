{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert TensorFlow models to TensorFlow Lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "[1] [https://www.tensorflow.org/lite/models/convert/](https://www.tensorflow.org/lite/models/convert/)\n",
    "\n",
    "[2] [https://www.tensorflow.org/lite/models/convert/convert_models](https://www.tensorflow.org/lite/models/convert/convert_models)\n",
    "\n",
    "[3] [https://www.tensorflow.org/lite/performance/post_training_quantization](https://www.tensorflow.org/lite/performance/post_training_quantization)\n",
    "\n",
    "[4] [https://www.tensorflow.org/lite/guide/inference](https://www.tensorflow.org/lite/guide/inference)\n",
    "\n",
    "[5] [https://www.tensorflow.org/lite/performance/model_optimization](https://www.tensorflow.org/lite/performance/model_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get overview of provided models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2_Kaggle_p150_e10\n",
      "InceptionV3_Kaggle_p150_e10\n",
      "model_p100_e5\n",
      "model_p244_e15\n",
      "MobileNetV3_Kaggle_p150_e10\n",
      "BigTransfer_Kaggle_p224_e45\n",
      "ResNet50V2_Kaggle_p150_e10\n",
      "model_p244_e5\n",
      "model_p150_e10\n",
      "EfficientNetB3_Kaggle_p150_e10\n",
      "model_p244_e10\n",
      "my_large_model\n"
     ]
    }
   ],
   "source": [
    "# get current absolute path of parent folder of this file\n",
    "path = os.path.dirname(os.path.abspath('tf2tf-lite_converter'))\n",
    "\n",
    "# list available models stored in 'saved_model' (without hidden folders)\n",
    "x = os.listdir(path + '/saved_model')\n",
    "x = [i for i in x if not i.startswith('.')]\n",
    "print(*x, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select model to convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which saved model to convert\n",
    "model = 'MobileNetV3_Kaggle_p150_e10'\n",
    "\n",
    "# Set path to saved model directory\n",
    "saved_model_dir = os.path.abspath(path + '/saved_model/' + model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute model conversion to TF Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-01 17:48:39.924502: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-11-01 17:48:39.924627: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-01 17:48:57.262021: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2022-11-01 17:48:57.262048: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2022-11-01 17:48:57.263461: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /Users/philipp/neuefische/ds-capstone-for-the-birds/saved_model/MobileNetV3_Kaggle_p150_e10\n",
      "2022-11-01 17:48:57.283114: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2022-11-01 17:48:57.283127: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /Users/philipp/neuefische/ds-capstone-for-the-birds/saved_model/MobileNetV3_Kaggle_p150_e10\n",
      "2022-11-01 17:48:57.349237: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-11-01 17:48:57.362150: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-01 17:48:57.693871: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /Users/philipp/neuefische/ds-capstone-for-the-birds/saved_model/MobileNetV3_Kaggle_p150_e10\n",
      "2022-11-01 17:48:57.847657: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 584199 microseconds.\n",
      "2022-11-01 17:48:58.121043: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-11-01 17:48:58.491353: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/MobilenetV3large/Conv/Conv2D because it has fewer than 1024 elements (432).\n",
      "2022-11-01 17:48:58.491372: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/MobilenetV3large/expanded_conv/depthwise/BatchNorm/FusedBatchNormV3;model/MobilenetV3large/expanded_conv/depthwise/depthwise;model/MobilenetV3large/expanded_conv/project/Conv2D because it has fewer than 1024 elements (144).\n",
      "2022-11-01 17:48:58.491376: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/MobilenetV3large/expanded_conv/project/Conv2D because it has fewer than 1024 elements (256).\n",
      "2022-11-01 17:48:58.491381: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/MobilenetV3large/expanded_conv_1/depthwise/BatchNorm/FusedBatchNormV3;model/MobilenetV3large/expanded_conv_1/depthwise/depthwise because it has fewer than 1024 elements (576).\n",
      "2022-11-01 17:48:58.491386: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/MobilenetV3large/expanded_conv_2/depthwise/BatchNorm/FusedBatchNormV3;model/MobilenetV3large/expanded_conv_2/depthwise/depthwise;model/MobilenetV3large/expanded_conv_3/squeeze_excite/Conv_1/Conv2D because it has fewer than 1024 elements (648).\n"
     ]
    }
   ],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] # Dynamic range quantization (4x smaller, 2x-3x speedup)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "path_save = 'saved_model_tf-lite/' + model + '.tflite'\n",
    "with open(path_save, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99cf74314b42ac8a8c1da03c92f17d43111865473f1a5aa1cc3b81d76b6237d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
